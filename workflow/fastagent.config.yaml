# Please edit this configuration file to match your environment (on Windows).
# Examples in comments below - check/change the paths.
#
#
default_model: generic.qwen3:32b
generic:
  api_key: "ollama" # Default for Ollama, change as needed
  base_url: "http://localhost:11434/v1" # Default for Ollama

logger:
  type: file
  level: error
  truncate_tools: true

mcp:
  servers:
    filesystem:
      # On windows update the command and arguments to use `node` and the absolute path to the server.
      # Use `npm i -g @modelcontextprotocol/server-filesystem` to install the server globally.
      # Use `npm -g root` to find the global node_modules path.`
      # command: "node"
      # args: ["c:/Program Files/nodejs/node_modules/@modelcontextprotocol/server-filesystem/dist/index.js","."]
      command: "npx"
      args: ["-y", "@modelcontextprotocol/server-filesystem", "."]
    fetch:
      command: "uvx"
      args: ["mcp-fetch"]
    curl:
      command: "npx"
      args: ["@mcp-get-community/server-curl"]
    curl-2:
      # Docker-based MCP entry: runs the curl MCP server inside a transient node container via npx.
      # Uses current working dir as /workspace so the server can access files if needed.
      command: "docker"
      args:
        - "run"
        - "--rm"
        - "-i"
        - "-v"
        - "${PWD}:/workspace"
        - "-w"
        - "/workspace"
        - "node:20"
        - "npx"
        - "-y"
        - "@mcp-get-community/server-curl"
        - "."
